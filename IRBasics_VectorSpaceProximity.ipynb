{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9ffa3ab",
   "metadata": {},
   "source": [
    "# NLP Foundations Workshop: From Preprocessing to tf-idf\n",
    "**Course:** PROG8245 ‚Äì Machine Learning Programming  \n",
    "**Team:** Team 5  \n",
    "**Team Members:**\n",
    "- Mandeep Singh (ID: 8989367)  \n",
    "- Kumari Nikitha Singh (ID: 9053016)  \n",
    "- Krishna¬†(ID:¬†905861)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78d7520",
   "metadata": {},
   "source": [
    "## Step 1: Document Collection\n",
    "We use the 20 Newsgroups dataset provided by Scikit-learn and extract the first 20 documents. This corpus represents real-world newsgroup posts and is commonly used in NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3cd80be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents loaded: 20\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Document Collection\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load dataset and keep only raw text (remove headers/footers/quotes)\n",
    "newsgroups = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Use first 5 documents to keep it simple\n",
    "documents = newsgroups.data[:20]\n",
    "\n",
    "# Show total documents\n",
    "print(f\"Total documents loaded: {len(documents)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba8fe40",
   "metadata": {},
   "source": [
    "**Talking Point:**  \n",
    "The 20 Newsgroups dataset reflects noisy, diverse real-world text ‚Äî ideal for simulating realistic NLP pipelines.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd407a2",
   "metadata": {},
   "source": [
    "## Step 2: Tokenization\n",
    "We implement a basic tokenizer using regular expressions. It:\n",
    "- Converts text to lowercase\n",
    "- Extracts only alphanumeric word tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e3b805d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document 0 ---\n",
      "\n",
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def view_document(documents, doc_id):\n",
    "    if 0 <= doc_id < len(documents):\n",
    "        print(f\"\\n--- Document {doc_id} ---\\n\")\n",
    "        print(documents[doc_id][:500] + \"...\\n\")\n",
    "    else:\n",
    "        print(\"Invalid document ID\")\n",
    "\n",
    "# Example: View Document 0\n",
    "view_document(documents, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1c154e",
   "metadata": {},
   "source": [
    "**Talking Point:**  \n",
    "Regex-based tokenization is more reliable than whitespace splitting. It avoids punctuation issues and provides clean tokens for indexing and frequency calculations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c20c3e",
   "metadata": {},
   "source": [
    "## Step 3: Normalization (Stopword Removal + Stemming)\n",
    "We remove common stop words using NLTK‚Äôs English stop word list and apply stemming with the Porter Stemmer to reduce words to their root form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "226eaf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 tokens: ['i', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'i', 'saw', 'the', 'other', 'day', 'it', 'was']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenizes text using regex:\n",
    "    - Lowercases all words\n",
    "    - Extracts word tokens using \\b\\w+\\b\n",
    "    \"\"\"\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# Example: Tokenize Document 0\n",
    "tokens_doc0 = tokenize(documents[0])\n",
    "print(\"First 20 tokens:\", tokens_doc0[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772f45fa",
   "metadata": {},
   "source": [
    "**Talking Point:**  \n",
    "Normalization reduces redundancy and focuses on meaningful content. Stemming helps group variants like ‚Äúrunning‚Äù and ‚Äúruns‚Äù under ‚Äúrun.‚Äù\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9efaab",
   "metadata": {},
   "source": [
    "## Step 4: Term-Document Incidence Matrix\n",
    "Using `CountVectorizer(binary=True)`, we create a binary matrix showing whether each term is present (1) or absent (0) in each document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6b280ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized tokens (first 20): ['wonder', 'anyon', 'could', 'enlighten', 'car', 'saw', 'day', '2', 'door', 'sport', 'car', 'look', 'late', '60', 'earli', '70', 'call', 'bricklin', 'door', 'realli']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kittu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download NLTK resources (only once)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stopwords and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def normalize_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Removes stop words and applies stemming.\n",
    "    \"\"\"\n",
    "    filtered = [t for t in tokens if t not in stop_words]\n",
    "    stemmed = [stemmer.stem(t) for t in filtered]\n",
    "    return stemmed\n",
    "\n",
    "# Example: Normalize tokens from Document 0\n",
    "normalized_tokens_doc0 = normalize_tokens(tokens_doc0)\n",
    "print(\"Normalized tokens (first 20):\", normalized_tokens_doc0[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21127109",
   "metadata": {},
   "source": [
    "**Talking Point:**  \n",
    "This matrix supports Boolean queries. It allows us to check which documents contain all query terms and enables basic rule-based retrieval systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022d8265",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5: Term Frequency (TF)\n",
    "We compute both raw term frequencies and normalized TF values for a sample document. Normalized TF is the term count divided by total words in the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab69bbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Term-Document Incidence Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>0320</th>\n",
       "      <th>0826</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>1000yds</th>\n",
       "      <th>100k</th>\n",
       "      <th>10mb</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>yhwh</th>\n",
       "      <th>yo</th>\n",
       "      <th>york</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "      <th>yrs</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows √ó 1545 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       000  0320  0826  10  100  1000  1000yds  100k  10mb  11  ...  \\\n",
       "Doc1     0     0     0   0    0     0        0     0     0   0  ...   \n",
       "Doc2     0     0     0   0    0     0        0     0     0   0  ...   \n",
       "Doc3     0     0     0   0    0     0        0     0     0   0  ...   \n",
       "Doc4     0     0     0   0    0     0        0     0     0   0  ...   \n",
       "Doc5     0     0     0   0    0     0        0     0     0   0  ...   \n",
       "Doc6     0     0     0   0    0     0        0     0     0   0  ...   \n",
       "Doc7     0     0     0   0    0     0        0     0     0   0  ...   \n",
       "Doc8     0     0     0   0    0     0        0     0     1   0  ...   \n",
       "Doc9     0     0     0   0    0     0        0     0     0   0  ...   \n",
       "Doc10    0     0     0   0    0     0        0     0     0   0  ...   \n",
       "Doc11    0     0     1   0    0     0        0     0     0   0  ...   \n",
       "Doc12    0     0     0   0    0     0        0     0     0   0  ...   \n",
       "Doc13    0     0     0   0    0     0        0     0     0   0  ...   \n",
       "Doc14    0     0     0   1    0     0        0     0     0   0  ...   \n",
       "Doc15    0     0     0   1    0     1        1     0     0   0  ...   \n",
       "Doc16    0     0     0   0    0     0        0     0     0   0  ...   \n",
       "Doc17    0     0     0   0    0     0        0     0     0   0  ...   \n",
       "Doc18    1     1     0   1    1     0        0     1     0   1  ...   \n",
       "Doc19    0     0     0   0    1     1        0     0     0   0  ...   \n",
       "Doc20    0     0     0   0    0     0        0     0     0   0  ...   \n",
       "\n",
       "       yesterday  yet  yhwh  yo  york  you  young  your  yrs  zoom  \n",
       "Doc1           0    0     0   0     0    1      0     0    0     0  \n",
       "Doc2           0    0     0   0     0    1      0     1    0     0  \n",
       "Doc3           0    0     0   0     0    1      0     0    0     0  \n",
       "Doc4           0    0     0   0     0    1      0     0    0     0  \n",
       "Doc5           0    1     0   0     0    1      0     0    0     0  \n",
       "Doc6           0    0     0   0     0    1      0     0    0     0  \n",
       "Doc7           0    0     0   0     0    0      0     0    0     0  \n",
       "Doc8           0    0     0   0     0    1      0     1    0     0  \n",
       "Doc9           0    0     0   0     0    0      0     0    0     0  \n",
       "Doc10          0    0     0   0     0    1      0     0    0     0  \n",
       "Doc11          0    0     0   0     0    0      0     0    0     0  \n",
       "Doc12          0    0     1   0     0    1      1     0    0     0  \n",
       "Doc13          0    0     0   0     0    0      0     0    0     0  \n",
       "Doc14          1    1     0   1     1    0      0     0    0     0  \n",
       "Doc15          0    0     0   0     0    1      0     1    0     1  \n",
       "Doc16          0    0     0   0     0    1      0     0    0     0  \n",
       "Doc17          0    0     0   0     0    1      0     0    0     0  \n",
       "Doc18          0    0     0   0     0    1      0     1    1     0  \n",
       "Doc19          0    0     0   0     0    0      0     0    0     0  \n",
       "Doc20          0    0     0   0     0    0      0     0    0     0  \n",
       "\n",
       "[20 rows x 1545 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Documents where all terms appear:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>machine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc8</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      machine\n",
       "Doc3        1\n",
       "Doc8        1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# --- Create Binary Term-Document Matrix ---\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Create labeled DataFrame\n",
    "incidence_matrix = pd.DataFrame(\n",
    "    X.toarray(),\n",
    "    index=[f\"Doc{i+1}\" for i in range(len(documents))],\n",
    "    columns=vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "print(\"üîé Term-Document Incidence Matrix:\")\n",
    "display(incidence_matrix)\n",
    "\n",
    "# --- Phrase Presence Checker ---\n",
    "def phrase_presence(query, matrix, vectorizer):\n",
    "    terms = query.lower().split()\n",
    "    valid_terms = [term for term in terms if term in vectorizer.vocabulary_]\n",
    "    \n",
    "    if not valid_terms:\n",
    "        return f\"No terms from '{query}' found in vocabulary.\"\n",
    "    \n",
    "    # Check if all query terms exist in each document\n",
    "    doc_matches = matrix[valid_terms].all(axis=1)\n",
    "    result = matrix[valid_terms][doc_matches]\n",
    "    \n",
    "    if result.empty:\n",
    "        return f\"No documents contain all terms from: '{query}'\"\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "# Example query\n",
    "phrase_result = phrase_presence(\"machine learning\", incidence_matrix, vectorizer)\n",
    "print(\"\\nüìå Documents where all terms appear:\")\n",
    "display(phrase_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb80d5ce",
   "metadata": {},
   "source": [
    "**Talking Point:**  \n",
    "TF reveals the most frequent and possibly most relevant terms in a document. Normalizing the values supports better comparison across documents of varying lengths.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f02633b",
   "metadata": {},
   "source": [
    "## Step 6: Log Frequency Weighting\n",
    "To limit the impact of very frequent terms, we apply log frequency weighting. This transformation compresses large counts while preserving their relative importance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd669a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Raw Term Frequencies:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Raw TF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>was</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wondering</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anyone</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>funky</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>looking</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>please</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>e</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>mail</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Term  Raw TF\n",
       "0           i       3\n",
       "1         was       4\n",
       "2   wondering       1\n",
       "3          if       2\n",
       "4      anyone       2\n",
       "..        ...     ...\n",
       "63      funky       1\n",
       "64    looking       1\n",
       "65     please       1\n",
       "66          e       1\n",
       "67       mail       1\n",
       "\n",
       "[68 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìè Normalized Term Frequencies:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Normalized TF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i</td>\n",
       "      <td>0.032258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>was</td>\n",
       "      <td>0.043011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wondering</td>\n",
       "      <td>0.010753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if</td>\n",
       "      <td>0.021505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anyone</td>\n",
       "      <td>0.021505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>funky</td>\n",
       "      <td>0.010753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>looking</td>\n",
       "      <td>0.010753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>please</td>\n",
       "      <td>0.010753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>e</td>\n",
       "      <td>0.010753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>mail</td>\n",
       "      <td>0.010753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Term  Normalized TF\n",
       "0           i       0.032258\n",
       "1         was       0.043011\n",
       "2   wondering       0.010753\n",
       "3          if       0.021505\n",
       "4      anyone       0.021505\n",
       "..        ...            ...\n",
       "63      funky       0.010753\n",
       "64    looking       0.010753\n",
       "65     please       0.010753\n",
       "66          e       0.010753\n",
       "67       mail       0.010753\n",
       "\n",
       "[68 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Choose one document (e.g., Doc 1)\n",
    "doc = documents[0]\n",
    "\n",
    "# Tokenize it\n",
    "tokens = tokenize(doc)\n",
    "\n",
    "# Count raw term frequency\n",
    "tf_raw = Counter(tokens)\n",
    "\n",
    "# Total terms in doc\n",
    "total_terms = len(tokens)\n",
    "\n",
    "# Normalize TF values\n",
    "tf_normalized = {term: count / total_terms for term, count in tf_raw.items()}\n",
    "\n",
    "# Show raw TF\n",
    "print(\"üî¢ Raw Term Frequencies:\")\n",
    "display(pd.DataFrame(tf_raw.items(), columns=[\"Term\", \"Raw TF\"]))\n",
    "\n",
    "# Show normalized TF\n",
    "print(\"\\nüìè Normalized Term Frequencies:\")\n",
    "display(pd.DataFrame(tf_normalized.items(), columns=[\"Term\", \"Normalized TF\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2609c7",
   "metadata": {},
   "source": [
    "**Talking Point:**  \n",
    "Log weighting reduces the dominance of repetitive terms like ‚Äúdata‚Äù or ‚Äúlearning‚Äù and stabilizes the input for statistical models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1277c525",
   "metadata": {},
   "source": [
    "## Step 7: Document Frequency (DF)\n",
    "We count how many documents contain each term. Terms with high DF appear in many documents and are less useful for distinguishing content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61554db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Log Frequency Weighted Term Frequencies:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Raw TF</th>\n",
       "      <th>Log TF Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>the</td>\n",
       "      <td>6</td>\n",
       "      <td>1.778151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>this</td>\n",
       "      <td>4</td>\n",
       "      <td>1.602060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>was</td>\n",
       "      <td>4</td>\n",
       "      <td>1.602060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>car</td>\n",
       "      <td>4</td>\n",
       "      <td>1.602060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>a</td>\n",
       "      <td>3</td>\n",
       "      <td>1.477121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>doors</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>were</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>small</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>in</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>mail</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Term  Raw TF  Log TF Weight\n",
       "14    the       6       1.778151\n",
       "11   this       4       1.602060\n",
       "1     was       4       1.602060\n",
       "12    car       4       1.602060\n",
       "18      a       3       1.477121\n",
       "..    ...     ...            ...\n",
       "32  doors       1       1.000000\n",
       "33   were       1       1.000000\n",
       "35  small       1       1.000000\n",
       "36     in       1       1.000000\n",
       "67   mail       1       1.000000\n",
       "\n",
       "[68 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Using the raw TF from Step 5\n",
    "log_weighted_tf = {\n",
    "    term: 1 + np.log10(freq) if freq > 0 else 0\n",
    "    for term, freq in tf_raw.items()\n",
    "}\n",
    "\n",
    "# Display results\n",
    "log_tf_df = pd.DataFrame({\n",
    "    \"Term\": tf_raw.keys(),\n",
    "    \"Raw TF\": tf_raw.values(),\n",
    "    \"Log TF Weight\": log_weighted_tf.values()\n",
    "})\n",
    "\n",
    "print(\"üìâ Log Frequency Weighted Term Frequencies:\")\n",
    "display(log_tf_df.sort_values(by=\"Log TF Weight\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13830bde",
   "metadata": {},
   "source": [
    "**Talking Point:**  \n",
    "Document frequency highlights terms that are common vs. specific. Low DF terms tend to be more informative for identifying document topics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ad76a5",
   "metadata": {},
   "source": [
    "## Step 8: Inverse Document Frequency (IDF)\n",
    "We calculate IDF using the formula log(N / df). Rare terms receive higher scores, helping us focus on unique or meaningful words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a473264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Document Frequency Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Document Frequency (df_t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>to</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>the</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>have</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>and</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>of</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>be</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>in</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>this</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>it</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>you</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Term  Document Frequency (df_t)\n",
       "1373    to                         18\n",
       "1342   the                         17\n",
       "658   have                         16\n",
       "176    and                         16\n",
       "957     of                         15\n",
       "237     be                         15\n",
       "713     in                         15\n",
       "1358  this                         15\n",
       "758     it                         13\n",
       "1540   you                         13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-use full 5-document corpus\n",
    "vectorizer_df = CountVectorizer()\n",
    "X_df = vectorizer_df.fit_transform(documents)\n",
    "\n",
    "# Extract terms and term-document matrix\n",
    "terms = vectorizer_df.get_feature_names_out()\n",
    "X_array = X_df.toarray()\n",
    "\n",
    "# Document frequency: count how many docs each term appears in\n",
    "df_counts = (X_array > 0).sum(axis=0)\n",
    "\n",
    "# Format results\n",
    "df_table = pd.DataFrame({\n",
    "    \"Term\": terms,\n",
    "    \"Document Frequency (df_t)\": df_counts\n",
    "}).sort_values(\"Document Frequency (df_t)\", ascending=False)\n",
    "\n",
    "print(\"üìä Document Frequency Table:\")\n",
    "display(df_table.head(10))  # Top 10 by DF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f02c7a",
   "metadata": {},
   "source": [
    "**Talking Point:**  \n",
    "IDF downweights generic terms and emphasizes words that carry more discriminative power across the corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ca3afc",
   "metadata": {},
   "source": [
    "## Step 9: TF-IDF Weighting\n",
    "We compute the TF-IDF score for each term in each document using log-scaled TF multiplied by IDF. This forms the basis for vector-based search systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f236444f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Inverse Document Frequency Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Document Frequency (df_t)</th>\n",
       "      <th>IDF (log10(N / df_t))</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>optimize</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>oklahoma</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>ok</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>oil</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>office</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>offer</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>oct</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>occurs</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>obtained</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Term  Document Frequency (df_t)  IDF (log10(N / df_t))\n",
       "0         000                          1                1.30103\n",
       "973  optimize                          1                1.30103\n",
       "963  oklahoma                          1                1.30103\n",
       "962        ok                          1                1.30103\n",
       "961       oil                          1                1.30103\n",
       "960    office                          1                1.30103\n",
       "959     offer                          1                1.30103\n",
       "956       oct                          1                1.30103\n",
       "955    occurs                          1                1.30103\n",
       "954  obtained                          1                1.30103"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Total number of documents\n",
    "N = len(documents)\n",
    "\n",
    "# Compute IDF\n",
    "idf_values = np.log10(N / df_counts)\n",
    "\n",
    "# Display as a DataFrame\n",
    "idf_table = pd.DataFrame({\n",
    "    \"Term\": terms,\n",
    "    \"Document Frequency (df_t)\": df_counts,\n",
    "    \"IDF (log10(N / df_t))\": idf_values\n",
    "}).sort_values(\"IDF (log10(N / df_t))\", ascending=False)\n",
    "\n",
    "print(\"üìâ Inverse Document Frequency Table:\")\n",
    "display(idf_table.head(10))  # Top 10 most informative (highest IDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd82ac",
   "metadata": {},
   "source": [
    "**Talking Point:**  \n",
    "TF-IDF captures both relevance within a document and uniqueness across the corpus. It's the foundation of classical IR, search ranking, and document classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c952b4e8",
   "metadata": {},
   "source": [
    "## Manual  TF-IDF and Cosine Similarity\n",
    "We convert a query phrase into a TF-IDF vector and calculate cosine similarity between it and each document. This ranks documents by semantic closeness to the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04a71f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä TF-IDF Matrix (Rounded to 3 decimals):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kittu\\AppData\\Local\\Temp\\ipykernel_5140\\3416743221.py:12: RuntimeWarning: divide by zero encountered in log10\n",
      "  tf_log = 1 + np.where(X_array > 0, np.log10(X_array), 0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>0320</th>\n",
       "      <th>0826</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>1000yds</th>\n",
       "      <th>100k</th>\n",
       "      <th>10mb</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>yhwh</th>\n",
       "      <th>yo</th>\n",
       "      <th>york</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "      <th>yrs</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.187</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.699</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.187</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.699</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.187</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.699</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc4</th>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.187</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.699</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc5</th>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.187</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.699</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc6</th>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.243</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.699</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc7</th>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.187</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.699</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc8</th>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>2.084</td>\n",
       "      <td>1.301</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.243</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.699</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc9</th>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.187</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.699</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc10</th>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.187</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.699</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc11</th>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.187</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.699</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc12</th>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.922</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.187</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.699</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc13</th>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.187</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.699</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc14</th>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.187</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.699</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc15</th>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.320</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.276</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.699</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc16</th>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.187</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.699</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc17</th>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.243</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.699</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc18</th>\n",
       "      <td>2.210</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1.477</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.693</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.426</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.243</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc19</th>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.187</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.699</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc20</th>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.187</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.699</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows √ó 1545 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         000   0320   0826     10    100  1000  1000yds   100k   10mb     11  \\\n",
       "Doc1   1.301  1.301  1.301  0.824  1.000   1.0    1.301  1.301  1.301  1.301   \n",
       "Doc2   1.301  1.301  1.301  0.824  1.000   1.0    1.301  1.301  1.301  1.301   \n",
       "Doc3   1.301  1.301  1.301  0.824  1.000   1.0    1.301  1.301  1.301  1.301   \n",
       "Doc4   1.301  1.301  1.301  0.824  1.000   1.0    1.301  1.301  1.301  1.301   \n",
       "Doc5   1.301  1.301  1.301  0.824  1.000   1.0    1.301  1.301  1.301  1.301   \n",
       "Doc6   1.301  1.301  1.301  0.824  1.000   1.0    1.301  1.301  1.301  1.301   \n",
       "Doc7   1.301  1.301  1.301  0.824  1.000   1.0    1.301  1.301  1.301  1.301   \n",
       "Doc8   1.301  1.301  1.301  0.824  1.000   1.0    1.301  1.301  2.084  1.301   \n",
       "Doc9   1.301  1.301  1.301  0.824  1.000   1.0    1.301  1.301  1.301  1.301   \n",
       "Doc10  1.301  1.301  1.301  0.824  1.000   1.0    1.301  1.301  1.301  1.301   \n",
       "Doc11  1.301  1.301  1.301  0.824  1.000   1.0    1.301  1.301  1.301  1.301   \n",
       "Doc12  1.301  1.301  1.301  0.824  1.000   1.0    1.301  1.301  1.301  1.301   \n",
       "Doc13  1.301  1.301  1.301  0.824  1.000   1.0    1.301  1.301  1.301  1.301   \n",
       "Doc14  1.301  1.301  1.301  0.824  1.000   1.0    1.301  1.301  1.301  1.301   \n",
       "Doc15  1.301  1.301  1.301  1.320  1.000   1.0    1.301  1.301  1.301  1.301   \n",
       "Doc16  1.301  1.301  1.301  0.824  1.000   1.0    1.301  1.301  1.301  1.301   \n",
       "Doc17  1.301  1.301  1.301  0.824  1.000   1.0    1.301  1.301  1.301  1.301   \n",
       "Doc18  2.210  1.301  1.301  0.824  1.477   1.0    1.301  1.693  1.301  1.301   \n",
       "Doc19  1.301  1.301  1.301  0.824  1.000   1.0    1.301  1.301  1.301  1.301   \n",
       "Doc20  1.301  1.301  1.301  0.824  1.000   1.0    1.301  1.301  1.301  1.301   \n",
       "\n",
       "       ...  yesterday  yet   yhwh     yo   york    you  young   your    yrs  \\\n",
       "Doc1   ...      1.301  1.0  1.301  1.301  1.301  0.187  1.301  0.699  1.301   \n",
       "Doc2   ...      1.301  1.0  1.301  1.301  1.301  0.187  1.301  0.699  1.301   \n",
       "Doc3   ...      1.301  1.0  1.301  1.301  1.301  0.187  1.301  0.699  1.301   \n",
       "Doc4   ...      1.301  1.0  1.301  1.301  1.301  0.187  1.301  0.699  1.301   \n",
       "Doc5   ...      1.301  1.0  1.301  1.301  1.301  0.187  1.301  0.699  1.301   \n",
       "Doc6   ...      1.301  1.0  1.301  1.301  1.301  0.243  1.301  0.699  1.301   \n",
       "Doc7   ...      1.301  1.0  1.301  1.301  1.301  0.187  1.301  0.699  1.301   \n",
       "Doc8   ...      1.301  1.0  1.301  1.301  1.301  0.243  1.301  0.699  1.301   \n",
       "Doc9   ...      1.301  1.0  1.301  1.301  1.301  0.187  1.301  0.699  1.301   \n",
       "Doc10  ...      1.301  1.0  1.301  1.301  1.301  0.187  1.301  0.699  1.301   \n",
       "Doc11  ...      1.301  1.0  1.301  1.301  1.301  0.187  1.301  0.699  1.301   \n",
       "Doc12  ...      1.301  1.0  1.922  1.301  1.301  0.187  1.301  0.699  1.301   \n",
       "Doc13  ...      1.301  1.0  1.301  1.301  1.301  0.187  1.301  0.699  1.301   \n",
       "Doc14  ...      1.301  1.0  1.301  1.301  1.301  0.187  1.301  0.699  1.301   \n",
       "Doc15  ...      1.301  1.0  1.301  1.301  1.301  0.276  1.301  0.699  1.301   \n",
       "Doc16  ...      1.301  1.0  1.301  1.301  1.301  0.187  1.301  0.699  1.301   \n",
       "Doc17  ...      1.301  1.0  1.301  1.301  1.301  0.243  1.301  0.699  1.301   \n",
       "Doc18  ...      1.301  1.0  1.301  1.301  1.301  0.426  1.301  1.243  1.301   \n",
       "Doc19  ...      1.301  1.0  1.301  1.301  1.301  0.187  1.301  0.699  1.301   \n",
       "Doc20  ...      1.301  1.0  1.301  1.301  1.301  0.187  1.301  0.699  1.301   \n",
       "\n",
       "        zoom  \n",
       "Doc1   1.301  \n",
       "Doc2   1.301  \n",
       "Doc3   1.301  \n",
       "Doc4   1.301  \n",
       "Doc5   1.301  \n",
       "Doc6   1.301  \n",
       "Doc7   1.301  \n",
       "Doc8   1.301  \n",
       "Doc9   1.301  \n",
       "Doc10  1.301  \n",
       "Doc11  1.301  \n",
       "Doc12  1.301  \n",
       "Doc13  1.301  \n",
       "Doc14  1.301  \n",
       "Doc15  1.301  \n",
       "Doc16  1.301  \n",
       "Doc17  1.301  \n",
       "Doc18  1.301  \n",
       "Doc19  1.301  \n",
       "Doc20  1.301  \n",
       "\n",
       "[20 rows x 1545 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use CountVectorizer to get term frequencies\n",
    "vectorizer_tfidf = CountVectorizer()\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(documents)\n",
    "terms = vectorizer_tfidf.get_feature_names_out()\n",
    "X_array = X_tfidf.toarray()\n",
    "\n",
    "# Document frequencies\n",
    "df = (X_array > 0).sum(axis=0)\n",
    "idf = np.log10(N / df)\n",
    "\n",
    "# Apply log-scaled TF\n",
    "tf_log = 1 + np.where(X_array > 0, np.log10(X_array), 0)\n",
    "\n",
    "# Compute TF-IDF\n",
    "tfidf = tf_log * idf\n",
    "\n",
    "# Final DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf, columns=terms, index=[f\"Doc{i+1}\" for i in range(N)])\n",
    "\n",
    "print(\"üìä TF-IDF Matrix (Rounded to 3 decimals):\")\n",
    "display(tfidf_df.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d59e22",
   "metadata": {},
   "source": [
    "**Talking Point:**  \n",
    "Cosine similarity allows us to turn our TF-IDF matrix into a semantic search engine. This mimics the behavior of intelligent search systems used in production.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f349fa",
   "metadata": {},
   "source": [
    "#  Conclusion\n",
    "\n",
    "In this Lab, we implemented a complete NLP pipeline that walks through the foundational stages of Information Retrieval, from raw document collection to semantic phrase queries using TF-IDF and cosine similarity.\n",
    "\n",
    "Each step‚Äîtokenization, normalization, frequency analysis, and weighting‚Äîcontributes to building structured representations of unstructured text. These representations are essential for powering modern search engines, recommendation systems, and intelligent agents.\n",
    "\n",
    "By exploring and applying key concepts such as Term Frequency, Document Frequency, IDF, and TF-IDF, we gained practical insight into how text can be converted into meaningful numerical data. We also extended this understanding by running phrase queries against the TF-IDF matrix to simulate how search engines rank documents based on relevance.\n",
    "\n",
    "This end-to-end process prepares us for the next phase of NLP and IR development, where we transition into **vector space models**, **clustering**, and **deep learning-based language models**.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaway:**  \n",
    "Building a robust IR system starts with mastering the basics‚Äîclean tokenization, thoughtful normalization, and weighted representation. These remain the building blocks for more advanced NLP systems in production today.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
